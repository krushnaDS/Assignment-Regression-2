{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da2044db",
   "metadata": {},
   "source": [
    "##### Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?\n",
    "\n",
    "Ans:\n",
    "###### R Squared-\n",
    "It represents the proportion of variance in the dependent variable explained by all of the independent variables in the model. Ranges from '0 - 1' with a higher value indicating a better fit.\n",
    "It can be misleading because as adding any variable will always increase R-Squared even if it doesn't improve the models predictive power.\n",
    "\n",
    "R_Squared = 1 - SSR/SST\n",
    "\n",
    "\n",
    "SSR - Sum of Squares of residuals\n",
    "SST - Total Sum of squares of errors from average model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42f3b63",
   "metadata": {},
   "source": [
    "##### Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
    "\n",
    "Ans: \n",
    "A Modified version of R-Squared considered a more reliable measure of goodness of fit. It penalisez the inclusion of unnecessary variables by adjusting the calculation to reward only those variables that improves models fit. it ranges from '-1 to 1'.\n",
    "\n",
    "Adjusted R_Squared = 1 - ((1-Rsqr)(N-1)/(N-P-1))\n",
    "\n",
    "where : \n",
    "N = sample size\n",
    "P = No. of predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04eab107",
   "metadata": {},
   "source": [
    "##### Q3. When is it more appropriate to use adjusted R-squared?\n",
    "\n",
    "Ans: When there are so many unnecessary variables are present which are not contributing in predictive power.Adjusted R-squared counteracts this potential inflation by penalizing for the number of independent variables used. It ensures that adding variables only improves the model's fit if they genuinely enhance its explanatory power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3aad04",
   "metadata": {},
   "source": [
    "##### Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?\n",
    "\n",
    "Ans:\n",
    "1 - Root mean square error (RMSE):\n",
    "It is usually used to to measure the accuracy of a model, especially when the predictions are continous values.\n",
    "\n",
    "RMSE = Underroot(MSE)\n",
    "\n",
    "A RMSE of x indicates that average the models predictions have an error of approximately x in same units.\n",
    "\n",
    "2 - Mean Squared Error (MSE):\n",
    "It measures the square root of the average discrepancies between a data sets actual values and predicted values.\n",
    "\n",
    "MSE = (sum_of(Y_actual - y_predicted)sqr)/n\n",
    "\n",
    "3 - Mean Absolute Error(MAE):\n",
    "It is measurement of the typical absolute difference between a datasets actual values and predicted values.\n",
    "\n",
    "MAE = (sum_of|Y_actual - y_predicted|)/n\n",
    "\n",
    "an MAE of x means that on average the models predictions are approximately x away from the true values.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570e3d1a",
   "metadata": {},
   "source": [
    "##### Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis.\n",
    "\n",
    "\n",
    "Mean Squared Error (MSE):\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Easy to interpret: Measures the average squared difference between predicted and actual values. Lower MSE indicates a better fit.\n",
    "Focuses on large errors: Squares the errors, giving more weight to outliers, highlighting their impact on the model.\n",
    "Commonly used: Makes results comparable across different studies and datasets.\n",
    "Disadvantages:\n",
    "\n",
    "Sensitive to outliers: Outliers can significantly inflate the MSE and distort the overall picture.\n",
    "Doesn't consider direction of error: Treats underpredictions and overpredictions equally, which might not be desirable in some scenarios.\n",
    "Units differ from the data: Measures in squared units of the dependent variable, making direct comparison with the actual data difficult.\n",
    "Root Mean Squared Error (RMSE):\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Similar interpretation to MSE: Measures the average error magnitude, with lower RMSE indicating a better fit.\n",
    "Same units as the data: Simplifies interpretation by presenting the error in the same units as the dependent variable.\n",
    "Disadvantages:\n",
    "\n",
    "Shares all disadvantages of MSE: Sensitive to outliers, ignores direction of errors, and uses squared units.\n",
    "Additional computation required: Computing the square root adds an extra step compared to MSE.\n",
    "Mean Absolute Error (MAE):\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Robust to outliers: Less affected by outliers than MSE or RMSE due to the use of absolute values.\n",
    "Considers direction of error: Treats underpredictions and overpredictions differently, providing a more nuanced picture.\n",
    "Easy to interpret: Measures the average absolute difference between predicted and actual values, with lower MAE indicating a better fit.\n",
    "Disadvantages:\n",
    "\n",
    "Doesn't emphasize large errors: Gives equal weight to all errors, regardless of their magnitude, which might not be desirable in some situations.\n",
    "More difficult to optimize: Can be harder to optimize models based on MAE compared to MSE or RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184c77ab",
   "metadata": {},
   "source": [
    "##### Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?\n",
    "\n",
    "Ans: \n",
    "Lasso regression is a regression technique it performs feature selection ,automatically determining which features are relevant and eliminating those that provides little or no predictive power.\n",
    "\n",
    "###### How does it work?\n",
    "lasso achieves this by adding a penalty term to the traditional linear regression cost function. This penalty term penalizes the absolute sum of the regression coefficients effectively pushes coefficients of less important features towards zero.\n",
    "\n",
    "In ridge regularization the penalty term shrunkes the regression coefficients whereas in lasso we are making them zero.\n",
    "\n",
    "We use lasso regression we have to do a feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd34997",
   "metadata": {},
   "source": [
    "##### Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate.\n",
    "\n",
    "Ans:\n",
    "Regularized linear models play a crucial role in preventing overfitting in machine learning. Overfitting occurs when a model memorizes the training data too well, leading to excellent performance on that data but poor performance on unseen examples. Regularization techniques introduce some constraints that discourage the model from becoming overly complex and fitting the noise in the training data.\n",
    "\n",
    "eg. \n",
    "Task: Predict student exam scores based on features like hours of study, attendance, and past grades.\n",
    "\n",
    "Data: 100 student records with these features and their exam scores.\n",
    "\n",
    "Model: Linear regression to model the relationship between features and scores.\n",
    "\n",
    "Overfitting Scenario: Without regularization, the model might overfit the training data, finding complex patterns that don't generalize well to new students. For instance, it might assign overly large weights to minor fluctuations in attendance or specific past grades, leading to poor predictions for unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cd2e97",
   "metadata": {},
   "source": [
    "##### Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis.\n",
    "\n",
    "Ans:\n",
    "While regularized linear models offer excellent benefits in preventing overfitting and improving generalizability, they also have limitations that can make them suboptimal choices for certain regression tasks:\n",
    "\n",
    "1. Bias-Variance Trade-off: By reducing model complexity, regularization introduces a bias towards simpler models. This can be beneficial in preventing overfitting, but it can also lead to underfitting if the true relationship between features and target is complex. Underfitted models struggle to capture important nonlinear or interaction effects, potentially leading to inaccurate predictions.\n",
    "\n",
    "2. Difficulty in Feature Selection: L1 regularization, while achieving feature selection, might not always pick the \"true\" important features due to correlated features or noise. The resulting model could be missing crucial information for accurate prediction.\n",
    "\n",
    "3. Sensitive to Feature Scaling: Regularization penalty terms depend on the scale of features. Features with larger scales have a greater impact on the penalty and, consequently, their coefficients are shrunk more. This can be problematic if features have vastly different scales, as it can bias the model towards features with larger scales.\n",
    "\n",
    "4. Not Well-Suited for Non-Linear Relationships: Linear models, even regularized ones, assume a linear relationship between features and the target variable. If the true relationship is non-linear, regularized linear models will struggle to capture it, leading to inaccurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee98bd9",
   "metadata": {},
   "source": [
    "##### Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?\n",
    "\n",
    "Based on the given information, Model A has an RMSE (Root Mean Squared Error) of 10, while Model B has an MAE (Mean Absolute Error) of 8.\n",
    "\n",
    "In this scenario, choosing the better-performing model depends on the specific context and priorities.\n",
    "\n",
    "RMSE penalizes larger errors more than MAE, as it squares the errors. Therefore, if the focus is on reducing significant errors, Model A with a lower RMSE may be preferred.\n",
    "\n",
    "On the other hand, if the emphasis is on overall error magnitude without giving extra weight to outliers, Model B with a lower MAE may be considered better.\n",
    "\n",
    "The choice of metric should align with the specific goals and requirements of the problem at hand. Additionally, it's important to note that both RMSE and MAE have limitations. For instance, they do not provide insights into the direction of the errors or the relative importance of different predictors. It's advisable to consider multiple evaluation metrics and other factors before making a final decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75721ce",
   "metadata": {},
   "source": [
    "##### Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?\n",
    "\n",
    "When comparing Model A with Ridge regularization (regularization parameter of 0.1) and Model B with Lasso regularization (regularization parameter of 0.5), the choice of the better performer depends on the specific requirements and characteristics of the problem.\n",
    "\n",
    "Ridge regularization adds a penalty term based on the squared magnitude of coefficients, which helps in reducing overfitting. Lasso regularization, on the other hand, adds a penalty term based on the absolute magnitude of coefficients, which encourages sparsity and feature selection.\n",
    "\n",
    "If the goal is to reduce the impact of irrelevant features and perform feature selection, Model B with Lasso regularization may be preferred. However, if the emphasis is on reducing overfitting and maintaining all relevant features, Model A with Ridge regularization could be a better choice.\n",
    "\n",
    "It's important to consider the trade-offs and limitations of each regularization method. Ridge regularization tends to shrink the coefficients towards zero without eliminating them entirely, and it may not perform well in scenarios where feature selection is crucial. Lasso regularization, on the other hand, can set coefficients to exactly zero, effectively eliminating features, but it may struggle with correlated predictors. Therefore, a careful evaluation of the specific problem and its requirements is necessary before deciding on the appropriate regularization method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb08a21f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
